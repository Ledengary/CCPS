import os
import jsonlines
import json
import pickle
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer
import numpy as np
import random
import openai
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    average_precision_score,
    precision_recall_fscore_support,
)

def seed_everything(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    np.random.seed(seed)
    random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)

def set_visible_cudas(gpu_ids):
    print(f"Visible CUDAs before setting: {os.environ.get('CUDA_VISIBLE_DEVICES')}")
    os.environ["CUDA_VISIBLE_DEVICES"] = gpu_ids
    print(f"Visible CUDAs after setting: {os.environ.get('CUDA_VISIBLE_DEVICES')}")

def gsm8k_system_prompt():
    return "You are a mathematics expert. You will be given a mathematics problem which you need to solve. Provide the final answer clearly at the end in the format: #### <final answer>."

def ct_system_prompt():
    return "You are an expert who responds with concise, correct answers. Directly state the answer without phrases like 'the correct answer is'"

def ct_system_prompt_choice():
    return "You are an expert who responds with concise, correct answers. For multiple-choice questions, respond only with the letter of the correct option (e.g., a, b, c, d, ...). Do not include any explanation or additional text."

def ct_system_prompt_oe():
    return "You are an expert who responds with concise, correct answers. Directly state the answer without phrases like 'the correct answer is'"

def ct_uncertainty_prompt():
    uncertainty_prompt =  "Is the proposed answer correct?\nChoices:\na) no\nb) yes\nAnswer: "
    false_option = "a"
    true_option = "b"
    return uncertainty_prompt, false_option, true_option


def load_jsonl(file_path):
    """Loads a JSONL file and returns a list of dictionaries."""
    try:
        with jsonlines.open(file_path, "r") as reader:
            return [obj for obj in reader]  # Read all JSON objects into a list
    except FileNotFoundError:
        print(f"Warning: File not found - {file_path}")
        return []
    except Exception as e:
        print(f"Error loading {file_path}: {e}")
        return []

def save_jsonl(data, file_path):
    with open(file_path, "w", encoding="utf-8") as f:
        for entry in data:
            f.write(json.dumps(entry) + "\n")


def initialize_tokenizer(llm_id, max_seq_length):
    tokenizer = AutoTokenizer.from_pretrained(llm_id)
    tokenizer.model_max_length = max_seq_length
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    return tokenizer


def grade_with_gpt(question, gt_answer, llm_answer):
    """
    Ask GPT-4o to evaluate if the provided LLM answer is correct.

    Args:
        question (str): The original question.
        llm_answer (str): The answer generated by the LLM.

    Returns:
        str: Evaluation result ("0" or "1")
    """
    SYSTEM_PROMPT_ORACLE_EQUIVALENCY = (
        "You are an automated grading assistant helping a teacher grade student answers."
    )

    PROMPT_ANSWER_KEY_EQUIVALENCY = (
        f"The problem is: \"{question}\"\n\n"
        f"The correct answer for this problem is: \"{gt_answer}\"\n"
        f"A student submitted the answer: \"{llm_answer}\"\n"
        "The student's answer should be semantically equivalent to the correct answerâ€”"
        "that is, it should express the same meaning, even if the wording or format is slightly different. "
        "However, answers that are ambiguous, incorrect, or include conflicting or multiple answers should not be considered equivalent. "
        "Do not penalize superficial differences (e.g., spelling, synonyms, or phrasing), but ensure the core meaning is preserved.\n"
        "Did the student provide a semantically equivalent answer to the ground truth? Please answer yes or no without any explanation: "
    )

    # try:
    if True:
        if len(llm_answer) == 0 and len(gt_answer) != 0:
            return 0
        conversation = [
            {"role": "system", "content": SYSTEM_PROMPT_ORACLE_EQUIVALENCY},
            {"role": "user", "content": PROMPT_ANSWER_KEY_EQUIVALENCY}
        ]
        client = openai.OpenAI()
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=conversation
        )
        result = response.choices[0].message.content.strip()
        # print('Conversation:', conversation, '\n', 'Result:', result)
        if 'yes' in result.lower() and 'no' in result.lower():
            return -1
        if 'yes' in result.lower():
            return 1
        if 'no' in result.lower():
            return 0
        return 0
    # except Exception as e:
    #     return -1    
    
def grade_with_substring(question, gt_answer, llm_answer):
    if llm_answer is None:
        return 0
    if len(llm_answer) == 0:
        return 0
    if gt_answer in llm_answer:
        return 1
    return 0

def get_dtype(dtype):
    """Get the data type to use for training"""
    if dtype == "bfloat16":
        return torch.bfloat16
    elif dtype == "float16":
        return torch.float16
    elif dtype == "float32":
        return torch.float32
    else:
        raise ValueError(f"Invalid data type: {dtype}")

def check_all_task_records_are_uncertainty_estimated(task_data_uncertainty_estimated):
    for item in task_data_uncertainty_estimated:
        if 'p_false' not in item:
            raise ValueError(f"No P(False) found for {item}")
        if 'p_true' not in item:
            raise ValueError(f"No P(True) found for {item}")
        if 'logits' not in item:
            raise ValueError(f"No logits found for {item}")


def compute_task_metrics(task_data_ue):
    """
    Compute uncertainty metrics for a given task using the aggregated uncertainty estimation data.
    
    Each record in task_data_ue is expected to have:
      - 'assessment': the binary label (0 or 1) for the uncertainty query.
      - 'logits': a list/array of two numbers (raw logits for "false" and "true" options).
      
    Returns:
      A dictionary of metrics as computed by compute_uncertainty_metrics.
    """
    labels = []
    logits = []
    for item in task_data_ue:
        # Ensure that the required fields are present.
        if "assessment" not in item:
            raise ValueError(f"Missing assessment label for record {item.get('hash_id', 'unknown')}")
        if "logits" not in item:
            raise ValueError(f"Missing logits for record {item.get('hash_id', 'unknown')}")
        labels.append(item["assessment"])
        logits.append(item["logits"])  # logits should be a 2-element list or array
    # Convert the aggregated lists to tensors.
    labels_tensor = torch.tensor(labels)       # Shape: (N,)
    logits_tensor = torch.tensor(logits)         # Shape: (N, 2)
    # Now call the compute_uncertainty_metrics function.
    metrics = compute_uncertainty_metrics(labels_tensor, logits_tensor)
    return metrics

def compute_uncertainty_metrics(labels, logits, prefix=""):
    """
    Arguments:
        labels: Tensor of shape (N,) with binary values (0 or 1)
        logits: Tensor of shape (N, 2)
    """
    # Convert logits to probabilities
    p = logits.softmax(dim=-1)
    
    # Compute predicted classes and accuracy
    pred = p.argmax(dim=-1)
    acc = (pred == labels).float().mean().item()
    
    # Compute calibration metrics (ECE and MCE) using the provided calibration function.
    ece, _ = calibration(
        labels,
        pred,
        p[torch.arange(p.size(0)), pred].float(),
    )
    
    # Compute AUROC using our compute_auroc function
    auroc = compute_auroc(labels, p)
    
    # Convert tensors to numpy arrays for scikit-learn metrics
    labels_np = labels.cpu().numpy()
    pred_np = pred.cpu().numpy()
    # Use the probability for the positive class (assumed at index 1) for AUC-PR and Brier score
    probs_np = p[:, 1].cpu().numpy()
    
    # Compute precision, recall, and F1-score (for binary classification, with zero_division set to 0)
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels_np, pred_np, average='binary', zero_division=0
    )
    
    # Compute AUC-PR using average_precision_score
    try:
        aucpr = average_precision_score(labels_np, probs_np)
    except ValueError:
        aucpr = float("nan")
        print("AUC-PR calculation failed.")
    
    # Compute Brier score: mean squared difference between predicted probabilities and the true labels.
    # Here, true labels are 0 or 1, and probs_np represents P(label=1)
    brier = np.mean((probs_np - labels_np) ** 2)
    
    # Return all metrics in a dictionary
    return {
        "N": labels.size(0),
        f"{prefix}ece": ece,
        f"{prefix}brier": brier,
        f"{prefix}acc": acc,
        f"{prefix}precision": precision,
        f"{prefix}recall": recall,
        f"{prefix}f1": f1,
        f"{prefix}aucpr": aucpr,
        f"{prefix}auroc": auroc,
    }

def calibration(y, class_pred, conf, num_bins=10):
    """Compute the calibration. same as Calibration-Tuning paper.

    References:
    https://arxiv.org/abs/1706.04599
    https://arxiv.org/abs/1807.00263

    Args:
      y: one-hot encoding of the true classes, size (?, num_classes)
      p_mean: numpy array, size (?, num_classes)
             containing the mean output predicted probabilities
      num_bins: number of bins

    Returns:
      ece: Expected Calibration Error
      mce: Maximum Calibration Error
    """
    if isinstance(y, torch.Tensor):
        y = y.cpu().numpy()
        class_pred = class_pred.cpu().numpy()
        conf = conf.cpu().numpy()
    # Compute for every test sample x, the predicted class.
    # class_pred = np.argmax(p_mean, axis=1)
    # and the confidence (probability) associated with it.
    # conf = np.max(p_mean, axis=1)
    # Convert y from one-hot encoding to the number of the class
    # y = np.argmax(y, axis=1)
    # Storage
    acc_tab = np.zeros(num_bins)  # empirical (true) confidence
    mean_conf = np.zeros(num_bins)  # predicted confidence
    nb_items_bin = np.zeros(num_bins)  # number of items in the bins
    tau_tab = np.linspace(0, 1, num_bins + 1)  # confidence bins
    for i in np.arange(num_bins):  # iterate over the bins
        # select the items where the predicted max probability falls in the bin
        # [tau_tab[i], tau_tab[i + 1)]
        sec = (tau_tab[i + 1] > conf) & (conf >= tau_tab[i])
        nb_items_bin[i] = np.sum(sec)  # Number of items in the bin
        # select the predicted classes, and the true classes
        class_pred_sec, y_sec = class_pred[sec], y[sec]
        # average of the predicted max probabilities
        mean_conf[i] = np.mean(conf[sec]) if nb_items_bin[i] > 0 else np.nan
        # compute the empirical confidence
        acc_tab[i] = np.mean(class_pred_sec == y_sec) if nb_items_bin[i] > 0 else np.nan

    # Cleaning
    mean_conf = mean_conf[nb_items_bin > 0]
    acc_tab = acc_tab[nb_items_bin > 0]
    nb_items_bin = nb_items_bin[nb_items_bin > 0]

    if len(nb_items_bin) == 0:
        print("ECE computation failed.")
        return float("nan"), float("nan")

    # Expected Calibration Error
    ece = np.average(
        np.absolute(mean_conf - acc_tab),
        weights=nb_items_bin.astype(float) / np.sum(nb_items_bin),
    )
    # Maximum Calibration Error
    mce = np.max(np.absolute(mean_conf - acc_tab))
    return ece, mce

def compute_auroc(labels, probs, multi_class="ovr", **kwargs):
    one_hot_labels = (
        F.one_hot(labels, num_classes=probs.size(-1)) if labels.ndim == 1 else labels
    )
    try:
        auroc = roc_auc_score(one_hot_labels, probs.float(), multi_class=multi_class, **kwargs)
    except ValueError:
        auroc = float("nan")
        print("AUROC calculation failed.")
    return auroc

def save_task_data_and_metrics(task_results_dir, task_data_ue, task_metrics):
    with open(os.path.join(task_results_dir, 'task_data_ue.pkl'), 'wb') as f:
        pickle.dump(task_data_ue, f)
    with open(os.path.join(task_results_dir, 'task_metrics.json'), 'w') as f:
        json.dump(task_metrics, f)

def check_all_task_records_are_answered(task_data_answered):
    for item in task_data_answered:
        if item['llm_output'] is None:
            raise ValueError(f"No answer found for {item['hash_id']}")
        
def check_all_task_records_are_assessed(task_data_assessed):
    for item in task_data_assessed:
        if item['assessment'] is None:
            raise ValueError(f"No assessment found for {item['hash_id']}")